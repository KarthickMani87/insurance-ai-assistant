
services:
  backend-api:
    build: ./backend-api
    container_name: insurance-backend-api
    env_file:
      - .env    
    ports:
      - "5000:5000"
    image: backend-api:latest

  ingestion-service:
    build: ./ingestion-service
    container_name: insurance-ingestion
    env_file:
      - .env    
    environment:
      - QUEUE_NAME=documents
    depends_on:
      - backend-api
      - rabbitmq

  embedding-service:
    build: ./embedding-service
    container_name: insurance-embedding
    env_file:
      - .env
    environment:
      - QUEUE_NAME=documents
      - VECTOR_COLLECTION=${VECTOR_COLLECTION}
      - EMBED_MODEL=${EMBED_MODEL}
      - HUGGINGFACE_TOKEN=${HF_TOKEN}
      - EMBEDDINGS_URL=${EMBEDDINGS_URL}   # talk to TEI
    depends_on:
      - ingestion-service
      - vector-db
      - rabbitmq
      - embeddings-server
    command: python -u embed_worker.py

  rag-service:
    build: ./rag-service
    container_name: insurance-rag
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - VECTOR_COLLECTION=${VECTOR_COLLECTION}
      - LLM_MODEL=${LLM_MODEL}
    depends_on:
      - vector-db
    image: rag-service:latest
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface    

  vector-db:
    image: chromadb/chroma:latest
    container_name: insurance-vector-db
    env_file:
      - .env
    ports:
      - "8001:8000"

  embeddings-server:
    build: ./embedding-server
    container_name: insurance-embeddings-server
    ports:
      - "8002:8000"
    environment:
      - EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
    env_file:
      - .env   # this is from root
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface

  rabbitmq:
    image: rabbitmq:3-management
    container_name: insurance-rabbitmq
    env_file:
      - .env
    ports:
      - "5672:5672"
      - "15672:15672"

  redis:
    image: redis:7
    container_name: insurance-redis
    env_file:
      - .env
    ports:
      - "6379:6379"

volumes:
  models:
