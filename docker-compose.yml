version: "3.8"

services:
  backend-api:
    build: ./backend-api
    container_name: insurance-backend-api
    env_file:
      - .env    
    ports:
      - "5000:5000"

  ingestion-service:
    build: ./ingestion-service
    container_name: insurance-ingestion
    env_file:
      - .env    
    environment:
      - QUEUE_NAME=documents
    depends_on:
      - backend-api
      - rabbitmq

  embedding-service:
    build: ./embedding-service
    container_name: insurance-embedding
    environment:
      - QUEUE_NAME=documents
      - OLLAMA_URL=http://ollama:11434/api/embeddings
      - EMBED_MODEL=mxbai-embed-large
    depends_on:
      - ingestion-service
      - vector-db
      - rabbitmq

  rag-service:
    build: ./rag-service
    container_name: insurance-rag
    ports:
      - "8000:8000"
    depends_on:
      - vector-db
      - ollama

  vector-db:
    image: chromadb/chroma:latest
    container_name: insurance-vector-db
    ports:
      - "8001:8000"

  ollama:
    image: ollama/ollama:latest
    container_name: insurance-ollama
    environment:
      - OLLAMA_NO_GPU=1
      - OLLAMA_HOST=http://0.0.0.0:11434
      - MODEL_NAME=qwen2.5:3b-instruct
      - EMBED_MODEL=mxbai-embed-large
    volumes:
      - ollama:/root/.ollama
    ports:
      - "11434:11434"
    restart: always
    entrypoint:
      - /bin/sh
      - -lc
      - |
        set -e
        /bin/ollama serve &
        echo "Waiting for Ollama..."
        until ollama list >/dev/null 2>&1; do sleep 1; done
        echo "Pulling $$MODEL_NAME..."
        ollama pull "$$MODEL_NAME"
        echo "Pulling $$EMBED_MODEL..."
        ollama pull "$$EMBED_MODEL"
        echo "âœ… Models ready."
        wait

  rabbitmq:
    image: rabbitmq:3-management
    container_name: insurance-rabbitmq
    ports:
      - "5672:5672"     # AMQP protocol
      - "15672:15672"   # Management UI (http://localhost:15672)
    environment:
      - RABBITMQ_DEFAULT_USER=guest
      - RABBITMQ_DEFAULT_PASS=guest

volumes:
  ollama:
